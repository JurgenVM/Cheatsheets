#Hadoop

## Steps to run JAR
1. `chmod 777 <your jar>.jar`
2. `hadoop fs -mkdir VoteCountInput`
3. `hadoop fs -cp votes.txt VoteCountInput`
4. `hadoop jar <your jar>.jar VoteCountInput VoteCountOutput`
5. `hadoop fs -ls VoteCountOutput`
6. `hadoop fs -cat VoteCountOutput/part-r-00000`

## File & Folder commands
| Command								| Meaning
| :---									| :---
| `hadoop fs -<Unix commands>`			| Main rule
| `hadoop fs -ls` 						| Show content hadoop home folder
| `hadoop fs put votes.txt` 			| Upload file to hadoop cluster (hfds)
| `hadoop fs -tail votes.txt`			| Show last lines file
| `hadoop fs -cat votes.txt`			| Show full content
| `hadoop fs -cp votes.txt votes2.txt`	| Copy file
| `hadoop fs -mv votes.txt newname.txt`	| Move file
| `hadoop fs -rm newname.txt`			| Remove file
| `hadoop fs -mkdir myinput`			| Create folder
| `hadoop fs -du votes.txt`				| Get size of file / folder
| `hadoop fs -du -s /.`					| Get size of everything in the folder

## Admin stuff
| Command								| Meaning
| :---									| :---
| `hdfs dfsadmin`						| Get all possible commands for `hdfs dfsadmin`
| `hdfs dfsadmin -report`				| Get dfs report

## Configuration files
`/home/training/src/hadoop-common-project/hadoop-common/src/test/resources/`

| Command								| Meaning
| :---									| :---
| `core-site.xml`						| Configuration file of the Hadoop system
|										| `fs.s3.block.size` Storage block size
| `hdfs-site.xml`						| HDFS specific configuration parameters
| `mapred-size.xml`						| MapReduces configuration

## Required JAR
- `/usr/lib/hadoop/hadoop-common-2.0.0-cdh4.1.1.jar`
- `/usr/lib/hadoop-mapreduce/hadoop-mapreduce-client-core-2.0.0-cdh4.1.1.jar`
- `/usr/lib/hadoop-mapreduce/hadoop-mapreduce-jobclient-2.0.0-cdh4.1.1.jar`
- `/usr/lib/hadoop/lib/commons-cli-1.2.jar`
- `/usr/lib/hadoop/lib/commons-logging-1.1.1.jar`

-------------------------------------------------------------------------------------

# Hive
| Command											| Meaning
| :---												| :---
| `hive`											| Load hive shell
| `quit;`											| Exit shell
| `hive -e "HiveQL query"`							| Execute query
| `hive -e "select sighted from ufodata limit 5;"`	| Sample of 5 values sighted column
| `hive -e "select reported from ufodata"`			| Validate the contents of column
| `hive -f commands.hql`							| Execute file

**Load data in the database from hadoop filesystem.**<br />
`LOAD DATA INPATH '/tmp/ufo.tsv' OVERWRITE INTO TABLE ufodata;`

**Set right column separator**<br />
```CREATE TABLE ufodata(sighted string, reported string, sighting_location
string, shape string, duration string, description string)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t';```

**Create table from existing file**<br />
```CREATE EXTERNAL TABLE states(abbreviation string, full_name string)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LOCATION '/tmp/states';```

**Exporting query output**<br />
```INSERT OVERWRITE DIRECTORY '/tmp/out'
SELECT t1.sighted, t1.reported, t1.shape, t2.full_name
FROM ufodata t1 JOIN states t2
ON (LOWER(t2.abbreviation) = LOWER(substr(t1.sighting_location, (LENGTH(t1.sighting_location) -1))));```
- Output will be in `/tmp/out`
	- `hadoop fs -ls /tmp/out`
	- `hadoop fs -cat /tmp/out/000000_1 | head`

**Making partitioned UFO sighting table**<br />
```CREATE TABLE partufo(sighted string, reported string, sighting_location
string,shape string, duration string, description string)
PARTITIONED BY (year string)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t';```

```SET hive.exec.dynamic.partition=true;
SET hive.exec.dynamic.partition.mode=nonstrict;
INSERT OVERWRITE TABLE partufo partition (year)
SELECT sighted, reported, sighting_location, shape, duration,
description, SUBSTR(TRIM(sighted), 1,4) FROM ufodata;```

- Location of the partioned table
	- `/user/hive/warehouse/partufo/`

## HiveQL vs SQL

[Source](http://hortonworks.com/blog/hive-cheat-sheet-for-sql-users/)

| MySQL																		| HiveQL
| :---																		| :---
| `SELECT from_columns FROM table WHERE conditions;`						| `SELECT from_columns FROM table WHERE conditions;`
| `SELECT * FROM table;`													| `SELECT * FROM table;`
| `SELECT * FROM table WHERE rec_name = "v";`								| `SELECT * FROM table WHERE rec_name = "v";`
| `SELECT * FROM TABLE WHERE r1 = "a" AND r2 = "b";`						| `SELECT * FROM TABLE WHERE r1 = "a" AND r2 = "b";`
| `SELECT column_name FROM table;`											| `SELECT column_name FROM table;`
| `SELECT DISTINCT column_name FROM table;`									| `SELECT DISTINCT column_name FROM table;`
| `SELECT c1, c2 FROM table ORDER BY c2;`									| `SELECT c1, c2 FROM table ORDER BY c2;`
| `SELECT c1, c2 FROM table ORDER BY c2 DESC;`								| `SELECT c1, c2 FROM table ORDER BY c2 DESC;`
| `SELECT COUNT(*) FROM table;`												| `SELECT COUNT(*) FROM table;`
| `SELECT own, COUNT(*) FROM table GROUP BY own;`							| `SELECT own, COUNT(*) FROM table GROUP BY own;`
| `SELECT MAX(col_name) AS label FROM table;`								| `SELECT MAX(col_name) AS label FROM table;`
| `SELECT p.name, comment FROM p, e WHERE p.name = e.name;`					| `SELECT p.name, comment FROM p JOIN e ON (p.name = e.name)`

| MySQL							| HiveQL
| :---							| :---
| `USE database;`				| `USE database;`
| `SHOW DATABASES;`				| `SHOW DATABASES;`
| `SHOW TABLES;`				| `SHOW TABLES;`
| 								| `SHOW TABLES '.*data';`
| `DESCRIBE table;`				| `DESCRIBE (FORMATTED|EXTENDED) table;`
| `CREATE DATABASE db_name;`	| `CREATE DATABASE db_name;`
| `DROP DATABASE db_name;`		| `DROP DATABASE db_name (CASCADE);`

# Sqoop

**Required packages**
- mysql-server
- mysql

**Setup database**
```
mysql> create database hadooptest;
mysql> CREATE USER ' hadoopuser'@'localhost' IDENTIFIED BY 'password';
mysql> GRANT ALL PRIVILEGES ON *.* TO ' hadoopuser'@'localhost' WITH GRANT OPTION;
mysql> flush privileges;
```

**Load data in existing table**
```
mysql> load data local infile 'employees.tsv'
into table employees
fields terminated by '\t' lines terminated by ' \n';
```

**Exporting data from MySQL to HDFS**
```
sqoop import --connect jdbc:mysql://localhost/hadooptest
--username hadoopuser --password password --table employees
```
- `hadoop fs -ls employees`
- `hadoop fs -cat employees/part-m-00003` 

**Exporting data from MySQL into Hive**
`hive -e "show tables like 'employees'"` 

```
sqoop import --connect jdbc:mysql://localhost/hadooptest
--username hadoopuser --password password --table employees 
--hive-import --hive-table employees
```

**Selective import**
```
sqoop import --connect jdbc:mysql://localhost/hadooptest
--username hadoopuser --password password
--table employees --columns first_name,salary
--where "salary > 45000"
--hive-import --hive-table salary
```

**Import data from Hadoop into MySQL**

1. `hadoop fs -mkdir edata`
2. `hadoop fs -put newemployees.tsv edata/newemployees.tsv`

```
sqoop export --connect jdbc:mysql://localhost/hadooptest
--username hadoopuser --password password --table employees
--export-dir edata --input-fields-terminated-by '\t'
```

**Importing Hive data into MySQL**

1. Empty your table (`truncate employees`)
2. `hadoop fs â€“ls /user/hive/warehouse/employees`

```
sqoop export --connect jdbc:mysql://localhost/hadooptest
--username hadoopuser --password password --table employees
--export-dir /user/hive/warehouse/employees
--input-fields-terminated-by ' \001'
--input-lines-terminated-by ' \n'
```




























